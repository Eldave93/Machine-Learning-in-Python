{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Day0-Gzd2yzL",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams['animation.embed_limit'] = 30000000.0\n",
    "plt.rcParams['figure.dpi'] = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UBi3Ju-i9Yf",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "The term Support Vector Machines (SVM's) is sometimes used loosely to refer to three methods; maximal margin classifier, a support vector classifier, and a support vector machine. Each are an extension of the previous method, allowing them to be applied to a broader range of cases.\n",
    "\n",
    "Support Vector Machines (SVM) are a common discriminative algorithm, well suited to *complex* small- to medium sized datasets<sup>2</sup>, which aim to find a hyperplane that provides the maximum margin of separation between classes of objects. They can be used for both classification and regression.\n",
    "\n",
    "**NOTES**\n",
    "- Images from the Hands on machine learning (using the petal data)\n",
    "- some explanation from the python ML\n",
    "- structure of the Intro to stats learning & some algebra\n",
    "- Some algebra from Machine learning: a probabilitistic perspective.\n",
    "\n",
    "---\n",
    "1. James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An introduction to statistical learning. Vol. 112. New York: springer, 2013.\n",
    "2. Géron, A. (2017). Hands-on machine learning with Scikit-Learn and TensorFlow: concepts, tools, and techniques to build intelligent systems. \" O'Reilly Media, Inc.\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "\"Developed in the computer science community in the 1990s\"<sup>1</sup>\n",
    "\n",
    "\"generalisation of the maximal margin classifier\" which requires a linear boundary - SVC as an extension can be applied to a broader range of cases<sup>1</sup>\n",
    "\n",
    "---\n",
    "1. James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An introduction to statistical learning. Vol. 112. New York: springer, 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification\n",
    "## Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximal margin classifier which induces a linear decision boundary on the feature space. SVMs simply divide the space to outline a decision boundary. Lewts have a look at a decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "#vis_data = X_train[:,[feature_list.index(x_axis_label),\n",
    "#                      feature_list.index(y_axis_label)]]\n",
    "\n",
    "#pipe_svc_linear.fit(vis_data, y_train)\n",
    "\n",
    "#plot_decision_regions(vis_data,\n",
    "#                      y_train,\n",
    "#                      clf = pipe_svc_linear)\n",
    "\n",
    "#plt.xlabel(x_axis_label) \n",
    "#plt.ylabel(y_axis_label)\n",
    "#plt.xlim(0,.6)\n",
    "#plt.ylim(0,1.)\n",
    "\n",
    "#plt.savefig('svm_linear_boundary.png')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SVM context two classes are (perfectly) separable by a K − 1 dimensional hyperplane. In one dimension the separator is a point, in two dimensions - a line, in three - a plane and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[More description on what a Hyperplane is and demonstration]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A subset of training data, known as support vectors, are selected by an algorithm to compute the optimal separation hyperplane between classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If data can be linearly separated, then a 'hard margin' of separation can be used; whereby a point on the edge of a class is used as the support vector for the decision boundary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However this method is sensitive to outliers, so a more flexible method may be preferable, using a soft margin of separation to compute a hyperplane that still provides a maximum margin of separation, whilst still allowing for some errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Discussion of Maximising the Decision margin]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Margin\n",
    "\n",
    "Not always the data could be perfectly separated by a K − 1 dimensional hyperplane. To overcome this problem we could either tweak the constraints on the hyperplane to allow some points to be misclassified (soft margin) or alternatively we could transform the data to be separable by a hyperplane in another space (kernel method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Scikit-Learn's SVM class this can be controlled by the C hyperparameter; with a smaller C creating a wider boundary but with more margin violations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear\n",
    "A hyperplane does not need to be linear as the input feature space can be projected to higher dimensions using a kernel (e.g. radial basis kernel<sup>2,3</sup>), allowing a hyperplane to be fitted to split the data into classes. The data can then be mapped back into the original feature space to create a nonlinear separation boundary.\n",
    "\n",
    "---\n",
    "2. Cover, T. M. (1965). Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. IEEE transactions on electronic computers, (3), 326-334.\n",
    "3. Varsavsky, A., Mareels, I., & Cook, M. (2016). Epileptic seizures and the EEG: measurement, models, detection and prediction. CRC Press."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Describe Mercer's Theorem using Hands on Machine Learning]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Describe and demonstrate various kernels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three classes for SVM classification in Scikit-Learn (table below adapted from Géron(2017)<sup>1</sup>):\n",
    "\n",
    "| Class                | Time Complexity                                                                      | Out-of-core Support | Kernel Trick|\n",
    "|------------------------|-------------------------------------------------------------------------------------------|---------------------------------|--------------------|\n",
    "| LinearSVC       | 0(*m* x *n*)                                                                               | No                               | No                |\n",
    "| SGDClassifier | 0(*m* x *n*)                                                                               | Yes                              | No                |\n",
    "| SVC                  | 0(*m*<sup>2</sup> x *n*) to 0(*m*<sup>3</sup> x *n*) | No                               | Yes               |\n",
    "\n",
    "First lets make a pipeline with two steps:\n",
    "\n",
    "1. Standardize the features\n",
    "2. SVM\n",
    "\n",
    "For the SVM we'll just use the SVC and set the kernel to linear so we can compare the decision boundary to the logistic regression as we did before. The data in our examples is quite small so using SVC, although takes longer than the other two methods, is fine for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction and SVM\n",
    "In order to reduce a models complexity, run time, and potential for overfitting to the training data, dimension reduction techniques can be used. Broadly they can be grouped into methods that create a subset of the original set of features (Feature Selection) and methods that create new synthetic features through combining the original features and discarding less important ones (Feature Extraction). Essentially we want to remove \"uninformative infromation\" and retain useful bits<sup>1</sup>. If you have too many features, it may be that some of them are highly correlated and therefore redundant. Therefore we can either select just some of them, or compress them onto a lower dimensional subspace<sup>2</sup>.\n",
    "\n",
    "---\n",
    "1. Zheng, A., & Casari, A. (2018). Feature Engineering for Machine Learning: Principles and Techniques for Data Scientists. \" O'Reilly Media, Inc.\".\n",
    "\n",
    "2. Raschka, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "\n",
    "A computationally efficient method of selecting features is to use a filter method. Filter methods aim to remove features with a low potential to predict outputs; usually though univariate analysis before classification. A filter could be a threshold set on each features variance, or the correlation or mutual information between each feature and the response variable. Although filters are computationally efficient comparative to other feature selection methods, they are independent of the model chosen so should be used conservatively to ensure data is not removed that a model may find useful<sup>1</sup>.\n",
    "\n",
    "### Variance Threshold\n",
    "\n",
    "VarianceThreshold is a simple baseline approach to feature selection. It removes all features whose variance doesn’t meet some threshold.\n",
    "\n",
    "---\n",
    "\n",
    "1. Zheng, A., & Casari, A. (2018). Feature Engineering for Machine Learning: Principles and Techniques for Data Scientists. \" O'Reilly Media, Inc.\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_selection import VarianceThreshold\n",
    "#from itertools import compress\n",
    "#from collections import defaultdict\n",
    "#import pprint\n",
    "#pp = pprint.PrettyPrinter()\n",
    "#\n",
    "#sel = VarianceThreshold(threshold=.8)\n",
    "#sel.fit(X_train)\n",
    "#\n",
    "# get boolian list of what is kept and what not\n",
    "#keep_bool = sel.get_support()\n",
    "# get index of false values\n",
    "#remove_index = [i for i, x in enumerate(keep_bool) if not x]\n",
    "#\n",
    "# merge multiindex feature labels into 1 label list\n",
    "#feat_labels = reduced_features.columns\n",
    "#remove_list = list(feat_labels[remove_index])\n",
    "#\n",
    "#print(color.BOLD+color.UNDERLINE+'Features and Channels Removed ('+str(len(remove_index))+')\\n'+color.END)\n",
    "#pp.pprint(remove_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedded Methods\n",
    "\n",
    "Instead of being independent, feature selection methods can be embedded in the model training process. An example would be the l1 regularizer for linear models, which imposes a sparsity constraint on the model to ensure a model favours fewer features. These methods are efficient and specific to the chosen model, but are not as powerful at wrapper methods (discussed next)<sup>1</sup>.\n",
    "\n",
    "Below is just an example of how you could implement it in a pipeline using a Support Vector Machine.\n",
    "\n",
    "---\n",
    "\n",
    "1. Zheng, A., & Casari, A. (2018). Feature Engineering for Machine Learning: Principles and Techniques for Data Scientists. \" O'Reilly Media, Inc.\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper Methods\n",
    "Wrapper methods are also  specific to the chosen model as they directly optimise the accuracy of a classifier by trying subsets of features. This enables keeping features that are useful in combination with others, even if uninformative in isolation<sup>1</sup>. Wrapper methods are the most computationally expensive, especially when used with nonlinear classifiers.\n",
    "\n",
    "---\n",
    "\n",
    "1. Zheng, A., & Casari, A. (2018). Feature Engineering for Machine Learning: Principles and Techniques for Data Scientists. \" O'Reilly Media, Inc.\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imballanced Data and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class\n",
    "\n",
    "Tree-based classifiers are inherently multiclass whereas other machine learning algorithms are able to be extended to multi-class classification using techniques such as the One-versus-Rest or One-versus-One methods<sup>1</sup>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-vs-the-rest (or one-verses-all)** is were you train a classifier for each class and select the class from the classifier that outputs the highest score<sup>1</sup>. As each class is fitted against all other classes for each classifier, it is relatively interpretable<sup>2</sup>.\n",
    "\n",
    "---\n",
    "1. Géron, A. (2017). Hands-on machine learning with Scikit-Learn and TensorFlow: concepts, tools, and techniques to build intelligent systems. \" O'Reilly Media, Inc.\".\n",
    "2. https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.multiclass import OneVsRestClassifier\n",
    "#\n",
    "#multi_pipe_svc_rbf = Pipeline([\n",
    "#    ('scl', StandardScaler()),\n",
    "#    ('clf', OneVsRestClassifier(SVC(C=100,\n",
    "#                                    kernel='rbf',\n",
    "#                                    gamma = 'auto',\n",
    "#                                    class_weight = 'balanced',\n",
    "#                                    random_state=RANDOM_STATE)))])\n",
    "\n",
    "#multi_pipe_svc_rbf.fit(multi_X_train, multi_y_train)\n",
    "#print('Validation Accuracy: %.3f' % multi_pipe_svc_rbf.score(multi_X_val, multi_y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another strategy is to use a **OneVsOneClassifer**. This trains $N \\times (N-1) / 2$ classifiers by comparing each class against each other so when a prediction is made, the class that is selected the most is chosen<sup>1</sup> (we'll get more onto *Bagging* next week). It is useful where algorithms do not scale well with data size (such as SVM) because each training and prediction is only needed to be run on a small subset of the data for each classifer<sup>1,2</sup>.\n",
    "\n",
    "---\n",
    "1. Géron, A. (2017). Hands-on machine learning with Scikit-Learn and TensorFlow: concepts, tools, and techniques to build intelligent systems. \" O'Reilly Media, Inc.\".\n",
    "2. https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.multiclass import OneVsOneClassifier\n",
    "#\n",
    "#multi_pipe_svc_rbf = Pipeline([\n",
    "#    ('scl', StandardScaler()),\n",
    "#    ('clf', OneVsOneClassifier(SVC(C=100,\n",
    "#                                   kernel='rbf',\n",
    "#                                   gamma = 'auto',\n",
    "#                                   class_weight = 'balanced',\n",
    "#                                   random_state=RANDOM_STATE)))])\n",
    "#\n",
    "#multi_pipe_svc_rbf.fit(multi_X_train, multi_y_train)\n",
    "#print('Validation Accuracy: %.3f' % multi_pipe_svc_rbf.score(multi_X_val, multi_y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi_pipe_svc_rbf.fit(multi_vis_data, multi_y_train)\n",
    "#\n",
    "#plot_decision_regions(multi_vis_data,\n",
    "#                      multi_y_train,\n",
    "#                      clf = multi_pipe_svc_rbf)\n",
    "#\n",
    "#plt.xlabel(x_axis_label) \n",
    "#plt.ylabel(y_axis_label)\n",
    "#plt.xlim(0,.6)\n",
    "#plt.ylim(0,1.)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority Voting\n",
    "\n",
    "A group of classifiers don't have to all be SVM's. Indeed Scikitlearn has a VotingClassifier where multipule classification pipelines can be combined to create an even better classifier that aggregates predictions. This aggregation can be done by simply selecting the class label that has been predicted by the majority of the classifiers (more than 50% of votes) for 'hard voting'. Majority vote refers to binary class decisions but can be generalized to a multi-class setting using 'plurality voting'. Particular classifiers return the probability of a predicted class label via the predict_proba method and this can be used for 'soft voting' instead of class labels<sup>1</sup>.\n",
    "\n",
    "Ensemble methods work best when the predictors are as independent as possible, so one way of achiving this is to get diverse classifiers. This increases the chance they each make different types of errors which in combination will improve the overall accuracy<sup>2</sup>.\n",
    "\n",
    "As can be seen below the soft majority voter has better scores than the hard voting method and better than most other methods individually when all are on their default settings. Soft voting often achives a higher performance than hard voting because highly confident votes are given more weight<sup>2</sup>.\n",
    "\n",
    "**NOTE**\n",
    "- with some hyper-parameter optimisation its likely we could increase the performance of the soft-majority vote.\n",
    "\n",
    "---\n",
    "1. Raschka, Sebastian, and Vahid Mirjalili. Python Machine Learning, 2nd Ed. Packt Publishing, 2017\n",
    "2. Géron, A. (2017). Hands-on machine learning with Scikit-Learn and TensorFlow: concepts, tools, and techniques to build intelligent systems. \" O'Reilly Media, Inc.\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace the Tree with one of the other classifiers they have already learnt\n",
    "\n",
    "#%%time\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.ensemble import VotingClassifier\n",
    "#from sklearn.pipeline import Pipeline\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.svm import SVC\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
    "#from sklearn.decomposition import PCA\n",
    "#import timeit\n",
    "#from sklearn.model_selection import StratifiedKFold\n",
    "#from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, make_scorer\n",
    "#\n",
    "#clf1 = Pipeline([('scl', StandardScaler()),\n",
    "#                 ('clf', SVC(kernel='rbf', \n",
    "#                             gamma='auto',\n",
    "#                             random_state=RANDOM_STATE, \n",
    "#                             probability = True))])\n",
    "\n",
    "#clf2 = Pipeline([('scl', StandardScaler()),\n",
    "#                 ('clf', LogisticRegression(solver='liblinear',\n",
    "#                                            random_state=RANDOM_STATE))\n",
    "#])\n",
    "\n",
    "#clf3 = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "#\n",
    "#clf_labels = ['SVM', # Support Vector Machine\n",
    "#              'LR', # LogisticRegression\n",
    "#              'DT'] # Decision Tree\n",
    "#\n",
    "# Majority Rule Voting\n",
    "#hard_mv_clf = VotingClassifier(estimators=[(clf_labels[0],clf1),\n",
    "#                                           (clf_labels[1],clf2),\n",
    "#                                           (clf_labels[2],clf3)],\n",
    "#                              voting='hard')\n",
    "#\n",
    "#soft_mv_clf = VotingClassifier(estimators=[(clf_labels[0],clf1),\n",
    "#                                           (clf_labels[1],clf2),\n",
    "#                                           (clf_labels[2],clf3)],\n",
    "#                               voting='soft')\n",
    "#\n",
    "#clf_labels += ['Hard Majority Voting', 'Soft Majority Voting']\n",
    "#all_clf = [clf1, clf2, clf3, hard_mv_clf, soft_mv_clf]\n",
    "#\n",
    "#print(color.BOLD+color.UNDERLINE+'Validation Scores\\n'+color.END)\n",
    "#for clf, label in zip(all_clf, clf_labels):\n",
    "#    start = timeit.default_timer() # TIME STUFF\n",
    "#    \n",
    "#    clf.fit(X_train, y_train)\n",
    "#\n",
    "#    y_pred = clf.predict(X_val)\n",
    "#    scores = f1_score(y_val, y_pred)\n",
    "#    print(color.BOLD+label+color.END)\n",
    "#    print(\"Score: %0.3f\"\n",
    "#          % scores)\n",
    "    # TIME STUFF\n",
    "#    stop = timeit.default_timer()\n",
    "#    print(\"Run time:\", np.round((stop-start)/60,2),\"minutes\")\n",
    "#    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#from sklearn.metrics import roc_curve\n",
    "#from sklearn.metrics import auc\n",
    "#\n",
    "# remove the hard voting because doesnt have predict proba\n",
    "#del clf_labels[3], all_clf[3]\n",
    "#\n",
    "#colors = ['black', 'orange', 'blue', 'green']\n",
    "#linestyles = [':', '--', '-.', '-']\n",
    "#for clf, label, clr, ls \\\n",
    "#        in zip(all_clf,\n",
    "#               clf_labels, colors, linestyles):\n",
    "#\n",
    "    # assuming the label of the positive class is 1\n",
    "#    y_pred = clf.fit(X_train, \n",
    "#                          y_train).predict_proba(X_test)[:, 1]\n",
    "#    fpr, tpr, thresholds = roc_curve(y_true=y_test,\n",
    "#                                     y_score=y_pred)\n",
    "#    roc_auc = auc(x=fpr, y=tpr)\n",
    "#    plt.plot(fpr, tpr,\n",
    "#             color=clr,\n",
    "#             linestyle=ls,\n",
    "#             label='%s (auc = %0.2f)' % (label, roc_auc))\n",
    "#\n",
    "#plt.legend(loc='lower right')\n",
    "#plt.plot([0, 1], [0, 1],\n",
    "#         linestyle='--',\n",
    "#        color='gray',\n",
    "#         linewidth=2)\n",
    "\n",
    "#plt.xlim([-0.1, 1.1])\n",
    "#plt.ylim([-0.1, 1.1])\n",
    "#plt.grid(alpha=0.5)\n",
    "#plt.xlabel('False positive rate (FPR)')\n",
    "#plt.ylabel('True positive rate (TPR)')\n",
    "\n",
    "#plt.savefig(os.path.join(IMAGE_DIR, 'Pipeline_Rocs.png'), dpi=300)\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "authorship_tag": "ABX9TyMNuZgSJ2BiDt5YMpOB66EK",
   "collapsed_sections": [],
   "name": "Clustering.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
