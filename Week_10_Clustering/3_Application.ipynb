{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "proper-deposit",
   "metadata": {},
   "source": [
    "## Applications\n",
    "\n",
    "__TODO__\n",
    "- maybe come back to these in the applications section.\n",
    "\n",
    "- _\"A cancer researcher might assay gene expression levels in 100 patients with breast cancer. He or she might then look for subgroups among the breast cancer samples, or among the genes, in order to obtain a better understanding of the disease._ \n",
    "- _An online shopping site might try to identify groups of shoppers with similar browsing and purchase histories, as well as items that are of particular interest to the shoppers within each group. Then an individual shopper can be preferentially shown the items in which he or she is particularly likely to be interested, based on the purchase histories of similar shoppers._\n",
    "- _A search engine might choose what search results to display to a particular individual based on the click histories of other individuals with similar search patterns.\"_<sup>1</sup>\n",
    "---\n",
    "1. An Introduction to Statistical Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-singles",
   "metadata": {},
   "source": [
    "__Applications__\n",
    "- _\"For instance, suppose that we have a set of n observations, each with p features. The n observations could correspond to tissue samples for patients with breast cancer, and the p features could correspond to measurements collected for each tissue sample; these could be clinical measurements, such as tumor stage or grade, or they could be gene expression measurements. We may have a reason to believe that there is some heterogeneity among the n tissue samples; for instance, perhaps there are a few different unknown subtypes of breast cancer. Clustering could be used to find these subgroups. This is an unsupervised problem because we are trying to discover structure—in this case, distinct clusters—on the basis of a data set.\"_\n",
    "\n",
    "- _\"Another application of clustering arises in marketing. We may have access to a large number of measurements (e.g. median household income, occupation, distance from nearest urban area, and so forth) for a large number of people. Our goal is to perform market segmentation by identifying subgroups of people who might be more receptive to a particular form of advertising, or more likely to purchase a particular product. The task of performing market segmentation amounts to clustering the people in the data set.\"_<sup>1</sup>\n",
    "\n",
    "__NOTE__\n",
    "1. think I'll pick the marketing one as the main example for the project\n",
    "\n",
    "---\n",
    "1. An Introduction to Statistical Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-notebook",
   "metadata": {},
   "source": [
    "_\"In order to perform clustering, some decisions must be made._\n",
    " - _Should the observations or features first be standardized in some way? For instance, maybe the variables should be centered to have mean zero and scaled to have standard deviation one.\"_ Intro to Data Sciecnce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-creativity",
   "metadata": {},
   "source": [
    "## Feature Selection and Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-palmer",
   "metadata": {},
   "source": [
    "## Imballanced Data and Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-nomination",
   "metadata": {},
   "source": [
    "## Limits of K-Means\n",
    "- section currently from https://github.com/ageron/handson-ml2/blob/master/09_unsupervised_learning.ipynb\n",
    "\n",
    "_\"One potential disadvantage of K-means clustering is that it requires us to pre-specify the number of clusters K...Hierarchical clustering has an added advantage over K-means clustering in that it results in an attractive tree-based representation of the observations, called a dendrogram.\"_ Intro to stats learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)\n",
    "X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))\n",
    "X2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)\n",
    "X2 = X2 + [6, -8]\n",
    "X = np.r_[X1, X2]\n",
    "y = np.r_[y1, y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(X, y=None):\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
    "\n",
    "plot_clusters(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-belarus",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_good = KMeans(n_clusters=3, init=np.array([[-1.5, 2.5], [0.5, 0], [4, 0]]), n_init=1, random_state=42)\n",
    "kmeans_bad = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans_good.fit(X)\n",
    "kmeans_bad.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-library",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3.2))\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_decision_boundaries(kmeans_good, X)\n",
    "plt.title(\"Inertia = {:.1f}\".format(kmeans_good.inertia_), fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_decision_boundaries(kmeans_bad, X, show_ylabels=False)\n",
    "plt.title(\"Inertia = {:.1f}\".format(kmeans_bad.inertia_), fontsize=14)\n",
    "\n",
    "#save_fig(\"bad_kmeans_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-academy",
   "metadata": {},
   "source": [
    "Limit\n",
    "\n",
    "_\"clustering methods generally are not very robust to perturbations to the data. For instance, suppose that we cluster n observations, and then cluster the observations again after removing a subset of the n observations at random. One would hope that the two sets of clusters obtained would be quite similar, but often this is not the case!\"_ James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning (Vol. 112, p. 18). New York: springer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-nicaragua",
   "metadata": {},
   "source": [
    "# Hierarchical clustering\n",
    "_\"The choice of dissimilarity measure is very important, as it has a strong effect on the resulting dendrogram. In general, careful attention should be paid to the type of data being clustered and the scientific question at hand. These considerations should determine what type of dissimilarity measure is used for hierarchical clustering._\n",
    "\n",
    "_For instance, consider an online retailer interested in clustering shoppers based on their past shopping histories. The goal is to identify subgroups of similar shoppers, so that shoppers within each subgroup can be shown items and advertisements that are particularly likely to interest them. Suppose the data takes the form of a matrix where the rows are the shoppers and the columns are the items available for purchase; the elements of the data matrix indicate the number of times a given shopper has purchased a given item (i.e. a 0 if the shopper has never purchased this item, a 1 if the shopper has purchased it once, etc.) What type of dissimilarity measure should be used to cluster the shoppers? If Euclidean distance is used, then shoppers who have bought very few items overall (i.e. infrequent users of the online shopping site) will be clustered together. This may not be desirable. On the other hand, if correlation-based distance is used, then shoppers with similar preferences (e.g. shoppers who have bought items A and B but never items C or D) will be clustered together, even if some shoppers with these preferences are higher-volume shoppers than others. Therefore, for this application, correlation-based distance may be a better choice._\n",
    "\n",
    "_In addition to carefully selecting the dissimilarity measure used, one must also consider whether or not the variables should be scaled to have standard deviation one before the dissimilarity between the observations is computed. To illustrate this point, we continue with the online shopping example just described. Some items may be purchased more frequently than others; for instance, a shopper might buy ten pairs of socks a year, but a computer very rarely. High-frequency purchases like socks therefore tend to have a much larger effect on the inter-shopper dissimilarities, and hence on the clustering ultimately obtained, than rare purchases like computers. This may not be desirable. If the variables are scaled to have standard deviation one before the inter-observation dissimilarities are computed, then each variable will in effect be given equal importance in the hierarchical clustering performed. We might also want to scale the variables to have standard deviation one if they are measured on different scales; otherwise, the choice of units (e.g. centimeters versus kilometers) for a particular variable will greatly affect the dissimilarity measure obtained. It should come as no surprise that whether or not it is a good decision to scale the variables before computing the dissimilarity measure depends on the application at hand. An example is shown in Figure 10.14. We note that the issue of whether or not to scale the variables before performing clustering applies to K-means clustering as well._\" An Introduction to Statistical Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-mediterranean",
   "metadata": {},
   "source": [
    "## Limitations of Hierarchical clustering\n",
    "_\"The term hierarchical refers to the fact that clusters obtained by cutting the dendrogram at a given height are necessarily nested within the clusters obtained by cutting the dendrogram at any greater height. However, on an arbitrary data set, this assumption of hierarchical structure might be unrealistic. For instance, suppose that our observations correspond to a group of people with a 50–50 split of males and females, evenly split among Americans, Japanese, and French. We can imagine a scenario in which the best division into two groups might split these people by gender, and the best division into three groups might split them by nationality. In this case, the true clusters are not nested, in the sense that the best division into three groups does not result from taking the best division into two groups and splitting up one of those groups. Consequently, this situation could not be well-represented by hierarchical clustering. Due to situations such as this one, hierarchical clustering can sometimes yield worse (i.e. less accurate) results than K-means clustering for a given number of clusters.\"_ Intro to stats learning\n",
    "\n",
    "__TODO__\n",
    "- Use more stuff from here: https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch11/ch11.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "danish-blame",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8e27608fc6c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0max1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Generate data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstandard_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(14,5))\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(2)\n",
    "X = np.random.standard_normal((50,2))\n",
    "X[:25,0] = X[:25,0]+3\n",
    "X[:25,1] = X[:25,1]-4\n",
    "\n",
    "km1 = KMeans(n_clusters=2, n_init=20)\n",
    "km1.fit(X)\n",
    "km2 = KMeans(n_clusters=3, n_init=20)\n",
    "km2.fit(X)\n",
    "\n",
    "ax1.scatter(X[:,0], X[:,1], s=40, c=km1.labels_, cmap=plt.cm.prism) \n",
    "ax1.set_title('K-Means Clustering Results with K=2')\n",
    "ax1.scatter(km1.cluster_centers_[:,0], km1.cluster_centers_[:,1], marker='+', s=100, c='k', linewidth=2)\n",
    "\n",
    "ax2.scatter(X[:,0], X[:,1], s=40, c=km2.labels_, cmap=plt.cm.prism) \n",
    "ax2.set_title('K-Means Clustering Results with K=3')\n",
    "ax2.scatter(km2.cluster_centers_[:,0], km2.cluster_centers_[:,1], marker='+', s=100, c='k', linewidth=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-reasoning",
   "metadata": {},
   "source": [
    "# Validating the Clusters <a id='val'></a>\n",
    "\n",
    "_\"We really want to know whether the clusters that have been found represent true subgroups in the data, or whether they are simply a result of clustering the noise. For instance, if we were to obtain an independent set of observations, then would those observations also display the same set of clusters? This is a hard question to answer. There exist a number of techniques for assigning a p-value to a cluster in order to assess whether there is more evidence for the cluster than one would expect due to chance. However, there has been no consensus on a single best approach.\"_<sup>1</sup>\n",
    "\n",
    "---\n",
    "1. Introduction to stats learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-representation",
   "metadata": {},
   "source": [
    "__Notes__\n",
    "- [This](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation) page on the Scikit-Learn documentation goes over a number of cluster validation approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-albert",
   "metadata": {},
   "source": [
    "# Extra\n",
    "## Other Clustering Algorithms\n",
    "`Scikit-Learn` implements many more clustering algorithms so here is a brief overview from<sup>3</sup>:\n",
    "\n",
    "__Balanced Iterative Reducing and Clustering using Hierarchies (BIRCH)__\n",
    "- For large datasets\n",
    "- Can be faster, with similar results, to batch k-means provided features $<20$.\n",
    "- Builds a minimal tree structure to quickly assign instances to clusters without having to store them all in memory.\n",
    "\n",
    "__TODO__\n",
    "- continue from pg.259\n",
    "\n",
    "---\n",
    "3. Géron, A. (2019). Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow: Concepts, tools, and techniques to build intelligent systems. O'Reilly Media."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
