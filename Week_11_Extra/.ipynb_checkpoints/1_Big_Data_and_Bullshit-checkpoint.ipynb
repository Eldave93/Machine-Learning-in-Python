{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "superior-roads",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Big Data, Black Boxes, and Bullsh*t\n",
    "### Dr. David Elliott\n",
    "\n",
    "1.1. [Introduction](#intro)\n",
    "\n",
    "1.2. [Big Data and Bias](#bdb)\n",
    "\n",
    "1.3. [Black Boxes](#blackbox)\n",
    "\n",
    "1.4. [Algorithmic Ethics](#ethics)\n",
    "\n",
    "1.5. [Fixing Bias](#solutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-security",
   "metadata": {},
   "source": [
    "__Notes__\n",
    "- Essentially a guide to approaching the results from blackbox models\n",
    "\n",
    "__TODO__\n",
    "- Have a look through what to use from ethics week in intro to data science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-growth",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction <a id='intro'></a>\n",
    "\n",
    "> The Navy revealed the embryo of an electronic computer today that it expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.\n",
    "\n",
    "July 8, 1958, The New York Times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-veteran",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The embryo in question is a perceptron, a simple logical circuit designed to mimic a biological neuron.\n",
    "\n",
    "It takes a set of numerical values as inputs, and then spits out either a 0 or a 1.\n",
    "\n",
    "Connect enough of these perceptrons together in the right ways, and you can build:\n",
    "\n",
    "- a chess-playing computer, \n",
    "- a self-driving car, \n",
    "- an algorithm that translates speech. \n",
    "\n",
    "These circuits are the building blocks for the convolutional neural networks and deep learning technologies that appear in headlines daily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-turkish",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "__Notes__\n",
    "- See the notebook on Deep Learning for more of an introduction\n",
    "> - The inventor of the perceptron, Frank Rosenblatt, was a psychologist by training, with broad interests in astronomy and neurobiology. \n",
    ">    - He used a two-million-dollar IBM 704 computer to simulate his first perceptron. \n",
    ">    - He also had a knack for selling big ideas and described his work in grandiose terms. \n",
    ">    - His machine, he told The New York Times, would think the way that humans do and learn from experience. Someday, he predicted, his perceptrons would be able to recognize faces and translate speech in real time. Perceptrons would be able to assemble other perceptrons, and perhaps even attain consciousness. Someday they could become our eyes and ears in the universe, sent out beyond the bounds of Earth to explore the planets and stars on our behalf.<sup>2</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-samuel",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> ...will make possible a new generation of artificial intelligence [AI] systems that will perform some functions that humans do with ease: see, speak, listen, navigate, manipulate and control.\n",
    "\n",
    "December 28, 2013, The New York Times\n",
    "\n",
    "Though the computer hardware is vastly more powerful, the basic approach remains similar to how it was a half century ago."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-hundred",
   "metadata": {},
   "source": [
    "The hype hasn’t diminished<sup>2<sup>:\n",
    "\n",
    "- Newspapers gush about the latest breakthrough. \n",
    "- AI jobs are paying superstar salaries. \n",
    "- Tech firms are wooing away from campus professors with AI expertise. \n",
    "- Venture capital firms are throwing money at anyone who can say “deep learning” with a straight face.\n",
    "    \n",
    "_\"The promise of AI spurs economic activity and inspires exciting science fiction plots; but it also creates unreasonable expectations, drives irresponsible research in both industry and academia, threatens to extinguish any hope of personal privacy, and motivates misdirected policy. Researchers and technologists spend far too much time focusing on the sexy what-might-be, and far too little time on the important what-is.\"_<sup>2<sup>\n",
    "    \n",
    "> \"Policy makers [are] earnestly having meetings to discuss the rights of robots when they should be talking about discrimination in algorithmic decision making.\" \n",
    "\n",
    "Zachary Lipton, AI researcher at Carnegie Mellon University"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-spotlight",
   "metadata": {},
   "source": [
    "_\"There is a vast gulf between AI alarmism in the popular press, and the reality of where AI research actually stands.\"_<sup>2</sup>\n",
    "\n",
    "_\"[AI poses a] fundamental risk to the existence of human civilization.\"_ Elon Musk, 2017\n",
    "\n",
    "_\"AI Is Inventing Languages Humans Can’t Understand. Should We Stop It?\"_ Fast Company article\n",
    "\n",
    "> BOB THE BOT: \"I can can I I everything else.\"\n",
    ">\n",
    "> ALICE THE BOT: \"Balls have zero to me to me to me to me to me to me to me to me to.\"\n",
    ">\n",
    "> BOB: \"You I everything else.\"\n",
    ">\n",
    "> ALICE: \"Balls have a ball to me to me to me to me to me to me to me to me.\"\n",
    "\n",
    "The original Facebook blog post simply described a chatbot evolving the repetition of nonsensical sentences, which was dramatically distorted to a story about saving the human race. \n",
    "\n",
    "_\"There was no panic,\"_ one researcher said, _\"and the project hasn’t been shut down.\"_ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-shape",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- _\"The story described a Facebook research project gone awry. While trying to build a chatbot that could carry on a convincing conversation, researchers tried having computer algorithms train one another to speak. But the speech that the algorithms developed was nothing like human language. Fast Company reported that the researchers quickly shut down the project. Skynet was officially on its way to self-awareness, but disaster had been averted—or so the story, and many others like it, suggested.\"_<sup>2</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-owner",
   "metadata": {},
   "source": [
    "However, Rosenblatt deserves credit because many of his ambitious predictions have come true:\n",
    "\n",
    "- Facial recognition technology, \n",
    "- virtual assistants, \n",
    "- machine translation systems, \n",
    "- stock-trading bots \n",
    "\n",
    "...are all built using perceptron-like algorithms<sup>2<sup>.\n",
    "\n",
    "Most of the recent breakthroughs in machine learning are due to the masses of data available and the processing power to deal with it, rather than a fundamentally different approach. \n",
    "    \n",
    "However you cannot compensate for bad data, if someone tells you otherwise, they are bullshitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-joining",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Bullshit? <a id='what'></a>\n",
    "Jevin West and Carl Bergstrom define bullshit as<sup>2</sup>:\n",
    "\n",
    "> Bullshit involves language, statistical figures, data graphics, and other forms of presentation intended to persuade or impress an audience by distracting, overwhelming, or intimidating them with a blatant disregard for truth, logical coherence, or what information is actually being conveyed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-amsterdam",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "__Notes__\n",
    "\n",
    "- _\"G. A. Cohen, notes that a lot of bullshit—particularly of the academic variety—is meaningless and so cloaked in rhetoric and convoluted language that no one can even critique it. Thus for Cohen, bullshit is “unclarifiable unclarity.”\"_<sup>2</sup>\n",
    "- _\"Neil Postman’s dictum, “At any given time, the chief source of bullshit with which you have to contend is yourself.”_<sup>2</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-skiing",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Big Data and Bias <a id='bdb'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-atmosphere",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We live in an ever increasing quantified world, where everything is counted, measured, and analyzed<sup>2</sup>:\n",
    "\n",
    "- Internet companies track web use and predict purchases.\n",
    "- Smartphones count our steps, measure our calls, and trace our movements throughout the day. \n",
    "- “Smart appliances” monitor use and learn about daily routines. \n",
    "- Implanted medical devices continuously collect data and predict emergencies. \n",
    "- Cars upload performance data and driving habits. \n",
    "- Sensors and cameras are across our cities monitoring traffic, air quality, and pedestrian identities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-township",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We've moved from companies paying customers to complete servays to them recording what we do:\n",
    "\n",
    "> - Facebook knows whom we know; \n",
    "> - Google knows what we want to know. \n",
    "> - Uber knows where we want to go; \n",
    "> - Amazon knows what we want to buy. \n",
    "> - Match knows whom we want to marry; \n",
    "> - Tinder knows whom we want to be swiped by."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-perspective",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Numbers feel objective, but are easily manipulated. \n",
    "\n",
    "Numbers suggest precision and imply a scientific approach, appearing to have an existence separate from the humans reporting them.\n",
    "\n",
    "__[INSERT SOMETHING LIKE: BUT THEY AREN'T ALWAYS]__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-mineral",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "__Notes__\n",
    "\n",
    "- _\"It’s like the old joke:<sup>2</sup>_\n",
    "    > A mathematician, an engineer, and an accountant are applying for a job. They are led to the interview room and given a math quiz. The first problem is a warm-up: What is 2 + 2? The mathematician rolls her eyes, writes the numeral 4, and moves on. The engineer pauses for a moment, then writes “Approximately 4.” The accountant looks around nervously, then gets out of his chair and walks over to the fellow administering the test. “Before I put anything in writing,” he says in a low whisper, “what do you want it to be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-shanghai",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sampling Error\n",
    "\n",
    "Exact counts and exhaustive measurements are nearly always impossible.\n",
    "\n",
    "_\"In statistical analysis, we deal with this problem by investigating small samples of a larger group and using that information to make broader inferences\"_\n",
    "\n",
    "_\"If one measured only a half dozen men and took their average height, it would be easy to get a misleading estimate simply by chance. Perhaps you sampled a few unusually tall guys. Fortunately, with large samples things tend to average out, and sampling error will have a minimal effect on the outcome.\"_\n",
    "\n",
    "---\n",
    "West, Jevin D.; Bergstrom, Carl T.. Calling Bullshit (pp. 77-79, 105-107, 109). Penguin Books Ltd. Kindle Edition. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-hungary",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Measurement Error\n",
    "\n",
    "_Researchers might ask subjects to report their own heights, but men commonly exaggerate their heights—and shorter men exaggerate more than taller men._\n",
    "\n",
    "_When a measure becomes a target, it ceases to be a good measure...psychologist Donald Campbell independently proposed an analogous principle: The more any quantitative social indicator is used for social decision-making, the more subject it will be to corruption pressures and the more apt it will be to distort and corrupt the social processes it is intended to monitor. Campbell illustrated his principle with the case of standardized testing in education: Achievement tests may well be valuable indicators of general school achievement under conditions of normal teaching aimed at general competence. But when test scores become the goal of the teaching process, they both lose their value as indicators of educational status and distort the educational process in undesirable ways._\n",
    "\n",
    "__TODO__\n",
    "\n",
    "- Use the calvin and hobbs comic used at the start of [Calling Bullshit 5.6: Algorithmic Ethics](https://www.youtube.com/watch?v=4u6HGaXx90A&list=PLPnZfvKID1Sje5jWxt-4CSZD7bUI4gSPS&index=28)\n",
    "---\n",
    "West, Jevin D.; Bergstrom, Carl T.. Calling Bullshit (pp. 77-79, 105-107, 109). Penguin Books Ltd. Kindle Edition. \n",
    "\n",
    "West, Jevin D.; Bergstrom, Carl T.. Calling Bullshit (pp. 94-95). Penguin Books Ltd. Kindle Edition. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-trout",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Selection Bias\n",
    "\n",
    "_Selection bias arises when the individuals that you sample for your study differ systematically from the population of individuals eligible for your study._\n",
    "\n",
    "_\"what you see depends on where you look\"_\n",
    "\n",
    "_Suppose you decide to estimate people’s heights by going to the local basketball court and measuring the players. Basketball players are probably taller than average, so your sample will not be representative of the population as a whole, and as a result your estimate of average height will be too high._\n",
    "\n",
    "_\"It seems that people turn to Google when looking for help, and turn to Facebook when boasting about their lives.\"_\n",
    "\n",
    "__TODO__: insert husband google/facebook posts\n",
    "\n",
    "West, Jevin D.; Bergstrom, Carl T.. Calling Bullshit (pp. 77-79, 105-107, 109). Penguin Books Ltd. Kindle Edition. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-motivation",
   "metadata": {},
   "source": [
    "__Poor interpretation of outliers:__ \n",
    "\n",
    "_\"Outliers can significantly skew data. For example, when analyzing income in the United States, there are a few extremely wealthy individuals whose income can warp any calculation of averages. For this reason, a median value is often a more accurate representation of the larger population.\"_ https://mailchimp.com/resources/data-bias-causes-effects/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-iceland",
   "metadata": {},
   "source": [
    "__Cognitive biases:__ \n",
    "    \n",
    "_\"These are effective feelings towards a person or a group based on their perceived group membership. More than 180 human biases have been defined and classified by psychologists, and each can affect individuals we make decisions. These biases could seep into machine learning algorithms via either_\n",
    "- _designers unknowingly introducing them to the model_\n",
    "- _a training data set which includes those biases\"_ https://research.aimultiple.com/ai-bias/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-kitchen",
   "metadata": {},
   "source": [
    "_\"No algorithm, no matter how logically sound, can overcome flawed training data.\"_\n",
    "\n",
    "_\"good training data are often difficult and expensive to obtain. In addition, training data typically come from the real world—but the real world is full of human biases and their consequences. For various reasons, the glamorous side of machine learning research involves developing new algorithms or tweaking old ones. But what is more sorely needed is research on how to select appropriate, representative data. Advances in that domain would pay rich dividends.\"_\n",
    "\n",
    "---\n",
    "West, Jevin D.; Bergstrom, Carl T.. Calling Bullshit (p. 191). Penguin Books Ltd. Kindle Edition. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-craps",
   "metadata": {},
   "source": [
    "# Black Boxes <a id='blackbox'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-customs",
   "metadata": {},
   "source": [
    "_\"Because the data are central to these systems, one rarely needs professional training in computer science to spot unconvincing claims or problematic applications. Most of the time, we don’t need to understand the learning algorithm in detail. Nor do we need to understand the workings of the program that the learning algorithm generates. (In so-called deep learning models, no one—including the creators of the algorithm—really understands the workings of the program that algorithm generates.) All you have to do to spot problems is to think about the training data and the labels that are fed into the algorithm. Begin with bad data and labels, and you’ll get a bad program that makes bad predictions in return. This happens so often that there is an acronym for it in computer science: GIGO—garbage in, garbage out.\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-saudi",
   "metadata": {},
   "source": [
    "_\"The central theme of this book is that you usually don’t have to open the analytic black box in order to call bullshit on the claims that come out of it. Any black box used to generate bullshit has to take in data and spit results out...Most often, bullshit arises either because there are biases in the data that get fed into the black box, or because there are obvious problems with the results that come out. Occasionally the technical details of the black box matter, but in our experience such cases are uncommon. This is fortunate, because you don’t need a lot of technical expertise to spot problems with the data or results. You just need to think clearly and practice spotting the sort of thing that can go wrong.\"_\n",
    "\n",
    "West, Jevin D.; Bergstrom, Carl T.. Calling Bullshit (p. 42). Penguin Books Ltd. Kindle Edition. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-joshua",
   "metadata": {},
   "source": [
    "_\"According to Latour, scientific claims are typically built upon the output of metaphorical “black boxes,” which are difficult if not impossible for the reader to penetrate. These black boxes often involve the use of specialized and often expensive equipment and techniques that are time-consuming and unavailable, or are so broadly accepted that to question them represents a sort of scientific heresy.fn1\"_\n",
    "\n",
    "West, Jevin D.; Bergstrom, Carl T.. Calling Bullshit (p. 41). Penguin Books Ltd. Kindle Edition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-thompson",
   "metadata": {},
   "source": [
    "_\"If the data that go into the analysis are flawed, the specific technical details of the analysis don’t matter.One can obtain stupid results from bad data without any statistical trickery. And this is often how bullshit arguments are created, deliberately or otherwise. To catch this sort of bullshit, you don’t have to unpack the black box. All you have to do is think carefully about the data that went into the black box and the results that came out. Are the data unbiased, reasonable, and relevant to the problem at hand? Do the results pass basic plausibility checks? Do they support whatever conclusions are drawn?\"_\n",
    "\n",
    "West, Jevin D.; Bergstrom, Carl T.. Calling Bullshit (p. 43). Penguin Books Ltd. Kindle Edition. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-connecticut",
   "metadata": {},
   "source": [
    "## Bullshit AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-equality",
   "metadata": {},
   "source": [
    "__Example__\n",
    "\n",
    "__TODO__\n",
    "- maybe just link to this lecture because I would just be repeating it. Just boil it down about loooking at the training data and the outputs.\n",
    "\n",
    "- [Calling Bullshit 5.5: Criminal Machine Learning](https://www.youtube.com/watch?v=rga2-d1oi30&list=PLPnZfvKID1Sje5jWxt-4CSZD7bUI4gSPS&index=27)\n",
    "\n",
    "I like this example due to it highlighting the same BS psychology studies that were debunked years ago are coming back in parts of the ML litriture. Back then they used \"Science\" to justify their claims, now these modern researchers use terms such as \"Artificial Intelligence\", \"Big Data\", or \"Machine Learning\", to justify the same garbage in a new package.\n",
    "\n",
    "- https://www.callingbullshit.org/case_studies/case_study_criminal_machine_learning.html\n",
    "\n",
    "_In late 2016, engineering researchers Xiaolin Wu and Xi Zhang submitted an article titled “Automated Inference on Criminality Using Face Images” to a widely used online repository of research papers known as the arXiv. In their article, Wu and Zhang explore the use of machine learning to detect features of the human face that are associated with “criminality.” They claim that their algorithm can use simple headshots to distinguish criminals from noncriminals with high accuracy._\n",
    "\n",
    "_The notion that criminals are betrayed by their physiognomy is not a new one. In the nineteenth century, an Italian doctor named Cesare Lombroso studied the anatomy of hundreds of criminals. His aim was to develop a scientific theory of criminality. He proposed that people were born to be criminals or to be upstanding citizens. Born criminals, he postulated, exhibit different psychological drives and physical features. Lombroso saw these features as hearkening back to our subhuman evolutionary past...In his view, the shape of the jaw, the slope of the forehead, the size of the eyes, and the structure of the ear all contained important clues about a man’s moral composition._\n",
    "\n",
    "_Lombroso was wrong. None of his theories linking anatomy to moral character have a sound scientific basis. His ideas—many of which wrapped racist ideas of the time in a thin veneer of scientific language—were debunked in the first half of the twentieth century and disappeared from the field of criminology. But in the 2016 arXiv paper, Wu and Zhang revisit Lombroso’s program. Essentially, they aim to determine whether advanced computer vision can reveal subtle cues and patterns that Lombroso and his followers might have missed. To test this hypothesis, the authors use machine learning algorithms to determine what features of the human face are associated with “criminality.” Wu and Zhang claim that based on a simple headshot, their programs can distinguish criminal from noncriminal faces with nearly 90 percent accuracy. Moreover, they argue that their computer algorithms are free from the myriad biases and prejudices that cloud human judgment:_\n",
    "\n",
    "> _Unlike a human examiner/judge, a computer vision algorithm or classifier has absolutely no subjective baggages [sic], having no emotions, no biases whatsoever due to past experience, race, religion, political doctrine, gender, age, etc., no mental fatigue, no preconditioning of a bad sleep or meal. The automated inference on criminality eliminates the variable of meta-accuracy (the competence of the human judge/examiner) all together._\n",
    "\n",
    "_...To understand the paper, we need to look at the training set. A machine learning algorithm can be only as good as the training data that it is provided. Wu and Zhang collected over 1,800 photos of Chinese men aged 18 to 55, with no distinguishing facial hair, scars, or tattoos. About 1,100 of these were noncriminals. Their photographs were taken from a variety of sources on the World Wide Web, including job-based social networking sites and staff listings from professional firms. Just over 700 of the subjects were convicted criminals. Their photos were provided by police departments and taken from official pieces of identification, not from mugshots...We see two massive problems. The first is that the images of noncriminals were selected to cast the individuals in a positive light. By contrast, the images from the set of criminals are official ID photographs. While it is unclear exactly what this means, it’s safe to guess that these have been selected neither by the person depicted, nor with the aim of casting him in a favorable light.A second source of bias is that the authors are using photographs of convicted criminals. If there are facial differences between the two groups, we won’t know whether these differences are associated with committing crimes or with being convicted. Indeed, appearance seems to matter for convictions. A recent study reports that in the US, unattractive individuals are more likely to be found guilty in jury trials than their attractive peers.fn2 Thus while the authors claim that their algorithm is free of human biases, it could be picking up on nothing but these biases._\n",
    "\n",
    "_Having identified some potential problems with the data that go into the black box, we turn to the black box’s output. As we mentioned, the authors find that their algorithm can classify criminal faces within their data set with 90 percent accuracy. What are the facial features that it uses to discriminate? The algorithm finds that criminals have shorter distances between the inner corners of the eyes, smaller angles θ between the nose and the corners of the mouth, and higher curvature ρ to the upper lip...There’s a glaringly obvious explanation for the nose-mouth angle and the lip curvature. As one smiles, the corners of the mouth spread out and the upper lip straightens.If you look at the original research paper, you can see six example images from the training set. The criminals are frowning or scowling. The noncriminals are faintly smiling. Now we have an alternative—and far more plausible—hypothesis for the authors’ findings...noncriminals are smiling in their professional headshots, whereas criminals are not smiling in their government ID photographs. It appears that the authors have confused innate facial features with labile facial expressions. If so, their claims about detecting criminality are bullshit. They have not invented a criminality detector; they have invented a smile detector._\n",
    "\n",
    "_extraordinary claims require extraordinary evidence.The authors of this paper make the extraordinary claim that facial structure reveals criminal tendencies. Here we see that their findings can be explained by a much more reasonable hypothesis: People are more likely to be smiling in professional headshots than in government ID photographs. Notice that we established all of this without opening the black box. We didn’t have to look at the details of the machine learning algorithms at all, because the problem didn’t arise there. A machine learning algorithm is only as good as its training data, and these training data are fundamentally flawed. As is often the case, one does not need technical expertise in machine learning to call bullshit._\n",
    "\n",
    "West, Jevin D.; Bergstrom, Carl T.. Calling Bullshit (pp. 44-48). Penguin Books Ltd. Kindle Edition.\n",
    "\n",
    "__Additional Example__\n",
    "- https://www.callingbullshit.org/case_studies/case_study_ml_sexual_orientation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-bidder",
   "metadata": {},
   "source": [
    "_\"Instead, the machines create their own rules to make decisions—and these rules often make little sense to humans.fn9\"_\n",
    "\n",
    "West, Jevin D.; Bergstrom, Carl T.. Calling Bullshit (p. 199). Penguin Books Ltd. Kindle Edition. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-invalid",
   "metadata": {},
   "source": [
    "__Fooling Deep Nets__\n",
    "\n",
    "https://arxiv.org/pdf/1412.1897.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-situation",
   "metadata": {},
   "source": [
    "## Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-manitoba",
   "metadata": {},
   "source": [
    "_\"Complicated models do a great job of fitting the training data, but simpler models often perform better on the test data than more complicated models. The trick is figuring out just how simple of a model to use. If the model you pick is too simple, you end up leaving useful information on the table.\"_\n",
    "\n",
    "West, Jevin D.; Bergstrom, Carl T.. Calling Bullshit (p. 191). Penguin Books Ltd. Kindle Edition. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-canyon",
   "metadata": {},
   "source": [
    "_\"When machine learning algorithms key in on auxiliary features of this sort, they may do well at analyzing images exactly like the ones they were trained on, but they will not be able to generalize effectively to other contexts. John Zech and colleagues at California Pacific Medical Center wanted to investigate how well neural networks could detect pathologies such as pneumonia and cardiomegaly—enlarged heart—using X-ray images. The team found that their algorithms performed relatively well in hospitals where they were trained, but poorly elsewhere.\"_\n",
    "\n",
    "_\"It turned out that the machine was cueing on parts of the images that had nothing to do with the heart or lungs. For example, X-ray images produced by a portable imaging device had the word PORTABLE printed in the upper right corner—and the algorithm learned that this is a good indicator that a patient has pneumonia. Why? Because portable X-ray machines are used for the most severely ill patients, who cannot easily go to the radiology department of the hospital. Using this cue improved prediction in the original hospital. But it was of little practical value. It had little to do with identifying pneumonia, didn’t cue in on anything doctors didn’t already know, and wouldn’t work in a different hospital that used a different type of portable X-ray machine.\"_\n",
    "\n",
    "West, Jevin D.; Bergstrom, Carl T.. Calling Bullshit (p. 200). Penguin Books Ltd. Kindle Edition. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-freeware",
   "metadata": {},
   "source": [
    "\"_HOW MACHINES FAIL_\n",
    "\n",
    "_\"In 2009, Nature published an article that described a new method for predicting flu outbreaks based on the search terms people use when querying Google. Terms such as “fever,” “headache,” “flu symptoms,” and “pharmacies near me” could be used to track the spread of flu across the United States. Not only could these search frequencies and their geolocations be used to predict doctor visits, the method was both faster and cheaper than the epidemiological tracking methods employed by the Centers for Disease Control and Prevention (CDC). The paper generated tremendous excitement and was covered by nearly every major newspaper and media outlet. Tech evangelists touted the results as an example of how big data would change the world. University professors, including one of the authors of this book, discussed the paper in their courses. Startup companies based around data analytics inserted the Nature paper into their pitch decks. When you have Google-scale data, argued Wired editor Chris Anderson, “the numbers speak for themselves.” The scientific method was no longer necessary, he argued; the huge volumes of data would tell us everything we need to know. Data scientists didn’t need years of epidemiological training or clinicians to diagnose flu symptoms. They just need enough data to “nowcast”fn12 the flu and inform the CDC where to deliver Tamiflu. Or so we were told. In all the excitement, we forget that if it sounds too good to be true, it probably is. And it was. By 2014, the headlines had turned from celebratory to monitory: “Google and the Flu: How Big Data Will Help Us Make Gigantic Mistakes,” “Why Google Flu Is a Failure,” “What We Can Learn from the Epic Failure of Google Flu Trends.” The method worked reasonably well for a couple of years but, before long, the results started to miss the mark, not by a little but by a factor of two. Over time, the predictions continued to get worse. The results became so poor that Google axed the project and took down the Flu Trends website. In retrospect, the study was doomed from the beginning. There was no theory about what search terms constituted relevant predictors of flu, and that left the algorithm highly susceptible to chance correlations in timing. For example, “high school basketball” was one of the top 100 predictors of a flu outbreak, simply because the search terms “flu” and “high school basketball” both peak in the winter. Like Hollywood relationships, spurious correlations fall apart on accelerated time scales. Search behavior and digital environments change—for example, Google introduced its “suggest” feature after this study was conducted.fn13 Doing so likely altered people’s search behavior. If you start typing “I am feeling” and the suggest feature offers “feverish,” you may choose this term more often than other variants (e.g., “I am feeling hot”). This increases the frequency of certain queries, and in a vicious cycle they may become even more likely to be suggested by Google’s autocomplete feature. When the frequency of search queries changes, the rules that the algorithm learned previously may no longer be effective. If the Google Flu Trends algorithm had to predict flu cases for only the first two years, we would still be writing about its triumph. When asked to extend beyond this time period, it failed. Sound familiar? Yep, this is overfitting. The machine likely focused on irrelevant nuances of that time period. This is where the scientific method can help. It is designed to develop theory that hyper-focuses on the key elements driving the spread of the flu, while ignoring the inconsequential. Search terms might be good indicators of those key elements, but we need a theory to help us generalize beyond two years of predictions. Without theory, predictions based on data rely on mere correlations. When venturing into the black box, also consider the following. Many of the more complicated algorithms use dozens, hundreds, or even thousands of variables when making predictions. Google Flu Trends relied on forty-five key search queries that best predicted flu outbreaks. A machine learning system designed to detect cancer might look at a thousand different genes. That might sound like a good thing. Just add more variables; more data equals better predictions, right? Well, not exactly. The more variables you add, the more training data you need. We talked earlier about the cost of obtaining good training data. If you have ten thousand genes you want to incorporate into your model, good luck in finding the millions of example patients that you will need to have any chance of making reliable predictions. This problem with adding additional variables is referred to as the curse of dimensionality. If you add enough variables into your black box, you will eventually find a combination of variables that performs well—but it may do so by chance. As you increase the number of variables you use to make your predictions, you need exponentially more data to distinguish true predictive capacity from luck. You might find that by adding the win-loss record of the New York Yankees to your list of a thousand other variables, you can better predict the Dow Jones index over the previous three months. But you will likely soon discover that this prediction success was the result of a chance alignment in the data—nothing more. Ask that variable to predict the index for the next three months, and the success rate will fall precipitously. Researchers have not stopped trying to use data to help clinicians and solve health problems, nor should they. Researchers at Microsoft Research are using search queries from Bing to detect people with undiagnosed pancreatic cancer, hopefully learning from the Google Flu Trends mistakes. Provided that it is collected legally, with consent, and with respect for privacy, data is valuable for understanding the world. The problem is the hype, the notion that something magical will emerge if only we can accumulate data on a large enough scale. We just need to be reminded: Big data is not better; it’s just bigger. And it certainly doesn’t speak for itself. In 2014, TED Conferences and the XPrize Foundation announced an award for “the first artificial intelligence to come to this stage and give a TED Talk compelling enough to win a standing ovation from the audience.” People worry that AI has surpassed humans, but we doubt AI will claim this award anytime soon. One might think that the TED brand of bullshit is just a cocktail of sound-bite science, management-speak, and techno-optimism. But it’s not so easy. You have to stir these elements together just right, and you have to sound like you believe them. For the foreseeable future, computers won’t be able to make the grade. Humans are far better bullshitters.\"_\n",
    "\n",
    "West, Jevin D.; Bergstrom, Carl T.. Calling Bullshit (pp. 202-205). Penguin Books Ltd. Kindle Edition. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-lesson",
   "metadata": {},
   "source": [
    "# Algorithmic Ethics <a id='ethics'></a>\n",
    "\n",
    "__Todo__\n",
    "- Notes from Weapons of math destruction\n",
    "- talk about the work from teh women fired from Google\n",
    "- Oversight of algorithms needed\n",
    "\n",
    "_\"Machines are not free of human biases; they perpetuate them, depending on the data they’re fed. In the case of criminal sentencing, ProPublica and others have shown that algorithms currently in use are identifying black defendants as “future” criminals at nearly twice the rate as white defendants, which leads to differences in pretrial release, sentencing, and parole deals. Algorithmic lenders charge higher interest rates to both black and Latino applicants. Automated hiring software from some of the largest US employers such as Amazon have preferentially selected men over women. When we train machines to make decisions based on data that arise in a biased society, the machines learn and perpetuate those same biases.\"_\n",
    "\n",
    "West, Jevin D.; Bergstrom, Carl T.. Calling Bullshit (pp. 200-201). Penguin Books Ltd. Kindle Edition. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-survivor",
   "metadata": {},
   "source": [
    "_\"To address the major impact that algorithms have on human lives, researchers and policy makers alike have started to call for algorithmic accountability and algorithmic transparency. Algorithmic accountability is the principle that firms or agencies using algorithms to make decisions are still responsible for those decisions, especially decisions that involve humans. We cannot let people excuse unjust or harmful actions by saying “It wasn’t our decision; it was the algorithm that did that.” Algorithmic transparency is the principle that people affected by decision-making algorithms should have a right to know why the algorithms are making the choices that they do. But many algorithms are considered trade secrets.fn10\"_\n",
    "\n",
    "West, Jevin D.; Bergstrom, Carl T.. Calling Bullshit (p. 201). Penguin Books Ltd. Kindle Edition. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-shuttle",
   "metadata": {},
   "source": [
    "_\"If you remove names from résumés as a way of eliminating gender discrimination, you may be disappointed, as Amazon was, when the machine continues to preferentially choose men over women. Why? Amazon trained the algorithm on its existing résumés, and there are features on a résumé besides a name that can reveal one’s gender—such as a degree from a women’s college, membership in a women’s professional organization, or a hobby with skewed gender representation.\"_\n",
    "\n",
    "West, Jevin D.; Bergstrom, Carl T.. Calling Bullshit (p. 202). Penguin Books Ltd. Kindle Edition. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-teens",
   "metadata": {},
   "source": [
    "_\"A new class of algorithms, collectively known as adversarial machine learning, can fashion photorealistic faces of nonexistent people out of whole cloth. The fabricated images are stunningly good. This is a dangerous period for a technology: It is widely available but few people know it’s being used. To raise public awareness, we developed a website called WhichFaceIsReal.com.\"__\n",
    "\n",
    "West, Jevin D.; Bergstrom, Carl T.. Calling Bullshit (p. 35). Penguin Books Ltd. Kindle Edition. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-cover",
   "metadata": {},
   "source": [
    "_\"Technologically, the same artificial intelligence techniques used to detect fake news can be used to get around detectors, leading to an arms race of production and detection that the detectors are unlikely to win.\"_\n",
    "\n",
    "West, Jevin D.; Bergstrom, Carl T.. Calling Bullshit (p. 36). Penguin Books Ltd. Kindle Edition. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-raise",
   "metadata": {},
   "source": [
    "# Fixing bias <a id='solutions'></a>\n",
    "\n",
    "https://research.aimultiple.com/ai-bias/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-advertising",
   "metadata": {},
   "source": [
    "## Compliment rather than supplement humans\n",
    "\n",
    "Big data, machine learning are not meant to replace humans, just supplement.\n",
    "\n",
    "Use the flu trends lessone from this lecture: https://www.youtube.com/watch?v=X0XqnAqvyIk&list=PLPnZfvKID1Sje5jWxt-4CSZD7bUI4gSPS&index=25\n",
    "        \n",
    "Also put in a bit from \"Deep Medicine\" :D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-vertical",
   "metadata": {},
   "source": [
    "_\"When it comes to open-ended tasks involving judgment and discretion, there is still no substitute for human intervention. Identifying fake news, detecting sarcasm, creating humor—for now, these are areas in which machines fall short of their human creators. However, reading addresses is relatively simple for a computer. The digit classification problem—figuring out whether a printed digit is a one, a two, a three, etc.—is a classic application of machine learning.\"_\n",
    "\n",
    "West, Jevin D.; Bergstrom, Carl T.. Calling Bullshit (p. 186). Penguin Books Ltd. Kindle Edition. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-mistress",
   "metadata": {},
   "source": [
    "# Refuting Bullshit\n",
    "\n",
    "__TODO__\n",
    "- write notes from the chapter above\n",
    "\n",
    "West, Jevin D.; Bergstrom, Carl T.. Calling Bullshit (p. 264). Penguin Books Ltd. Kindle Edition. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-garbage",
   "metadata": {},
   "source": [
    "# What next?\n",
    "\n",
    "- If you want more deep learning, learn the keras api for Tensorflow 2.0 or PyTorch.\n",
    "- Once your happy with `scikit-learn` you may want to look at some related python projects: https://scikit-learn.org/stable/related_projects.html#related-projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-record",
   "metadata": {},
   "source": [
    "# Recommended Lectures\n",
    "\n",
    "__(\"Guest\" Lectures in the age of COVID)__\n",
    "- [Calling Bullshit 5.1: Big Data](https://www.youtube.com/watch?v=FLKzmswqF7E&list=PLPnZfvKID1Sje5jWxt-4CSZD7bUI4gSPS&index=23)\n",
    "- [Calling Bullshit 5.2: Garbage In, Garbage Out](https://www.youtube.com/watch?v=pcmUdXIJQ74&list=PLPnZfvKID1Sje5jWxt-4CSZD7bUI4gSPS&index=24)\n",
    "- [Calling Bullshit 5.3: Big Data Hubris](https://www.youtube.com/watch?v=X0XqnAqvyIk&list=PLPnZfvKID1Sje5jWxt-4CSZD7bUI4gSPS&index=25)\n",
    "- [Calling Bullshit 5.4: Overfitting](https://www.youtube.com/watch?v=pDyB_ufVyIw&list=PLPnZfvKID1Sje5jWxt-4CSZD7bUI4gSPS&index=26)\n",
    "- [Calling Bullshit 5.5: Criminal Machine Learning](https://www.youtube.com/watch?v=rga2-d1oi30&list=PLPnZfvKID1Sje5jWxt-4CSZD7bUI4gSPS&index=27)\n",
    "- [Calling Bullshit 5.6: Algorithmic Ethics](https://www.youtube.com/watch?v=4u6HGaXx90A&list=PLPnZfvKID1Sje5jWxt-4CSZD7bUI4gSPS&index=28)\n",
    "\n",
    "\n",
    "# Recommended Readings\n",
    "\n",
    "__Reading__<sup>1</sup>\n",
    "\n",
    "- danah boyd and Kate Crawford (2011) Six Provocations for Big Data. A Decade in Internet Time: Symposium on the Dynamics of the Internet and Society.\n",
    "- David Lazer et al. (2014) The Parable of Google Flu: Traps in Big Data Analysis. Science 343:1203-1205\n",
    "- Alyin Caliskan et al. (2017) Semantics derived automatically from language corpora contain human-like biases Science 356:183-186\n",
    "- Jevin West (2014) How to improve the use of metrics: learn from game theory. Nature 465:871-872\n",
    "\n",
    "__Supplementary reading__<sup>1</sup>\n",
    "\n",
    "- West, Jevin D.; Bergstrom, Carl T.. Calling Bullshit (p. 41). Penguin Books Ltd.\n",
    "- Cathy O'Neil (2016) Weapons of Math Destruction Crown Press.\n",
    "- Peter Lawrence (2014) The mismeasurement of science. Current Biology 17:R583-585\n",
    "\n",
    "# References\n",
    "1. https://www.callingbullshit.org/syllabus.html#Big\n",
    "2. West, Jevin D.; Bergstrom, Carl T.. Calling Bullshit, Penguin Books Ltd."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
