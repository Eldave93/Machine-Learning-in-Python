{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<script>\n",
    "    document.querySelector('head').innerHTML += '<style>.slides { zoom: 1. !important; }</style>';\n",
    "</script>\n",
    "\n",
    "# Week 9 - Tree-Based Methods\n",
    "### Dr. David Elliott\n",
    "\n",
    "1.8. [Hyperparameters](#hypers)\n",
    "    \n",
    "1.9. [Imballanced Data](#imb)\n",
    "    \n",
    "1.10. [Strengths and Limitations](#adv_lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "__TODO__\n",
    "- The following has good hyperparam advice for trees, bagging, and forests so use that! https://bradleyboehmke.github.io/HOML/random-forest.html\n",
    "- compare models together to demonstrate pros and cons\n",
    "- add stuff from pg. 552 in ML prob perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters <a id='hypers'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The important parameters to adjust are n_estimators, max_features, and possibly pre-pruning options like max_depth. For n_estimators, larger is always better. Aver‐ aging more trees will yield a more robust ensemble by reducing overfitting. However, there are diminishing returns, and more trees need more memory and more time to train. A common rule of thumb is to build “as many as you have time/memory for.” As described earlier, max_features determines how random each tree is, and a smaller max_features reduces overfitting. In general, it’s a good rule of thumb to use the default values: max_features=sqrt(n_features) for classification and max_fea tures=n_features for regression. Adding max_features or max_leaf_nodes might sometimes improve performance. It can also drastically reduce space and time requirements for training and prediction.\n",
    "\n",
    "Müller, A. C., & Guido, S. (2016). Introduction to machine learning with Python: a guide for data scientists. \" O'Reilly Media, Inc.\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imballanced Data <a id='imb'></a>\n",
    "\n",
    "__TODO__\n",
    "- discuss general ensembles for imballances as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Forest\n",
    "It is also worth noting we have been dealing with the class imballance found in this data by using `class_weight = 'balanced'` to assign more importance to getting ictal data predictions correct. We can however also undersample using a ballanced random forest. Generally what performs better depends on the amount of data you are training on. If small then class wight will be better (as seen below), but if you have very large datasets, then undersampling will likely work better.\n",
    "\n",
    "__NOTE__\n",
    "- note sure this imballance is going to do much, use abalone_data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 113, 1: 99})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(Counter(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[4mRandom Forest\u001b[0m\n",
      "CV accuracy: 0.995 +/- 0.010\n",
      "\u001b[1m\u001b[4mBalanced Random Forest\u001b[0m\n",
      "CV accuracy: 0.995 +/- 0.010\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "bal_f = BalancedRandomForestClassifier(criterion='gini',\n",
    "                                       n_estimators=1000,\n",
    "                                       max_features = 'sqrt',\n",
    "                                       random_state=42,\n",
    "                                       n_jobs=-1)\n",
    "\n",
    "rf_dict = {'Random Forest':rf, 'Balanced Random Forest':bal_f}\n",
    "\n",
    "for classifier_name in rf_dict:\n",
    "    scores = cross_val_score(estimator=rf_dict[classifier_name], \n",
    "                             X=X_train, \n",
    "                             y=y_train, \n",
    "                             scoring = 'accuracy',\n",
    "                             cv=StratifiedKFold(),\n",
    "                             n_jobs=-1)\n",
    "\n",
    "    print(color.BOLD+color.UNDERLINE+classifier_name+color.END)\n",
    "    #print('CV accuracy scores: %s' % scores)\n",
    "    print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strengths and Limitations <a id='adv_lim'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Pros__\n",
    "- Easy to explain\n",
    "    - Trees can be displayed graphically in an interpretable mannor.\n",
    "- Inherently multiclass\n",
    "- Can handle different types of predictors*.\n",
    "    - Independent of feature scaling\n",
    "\n",
    "__Cons__\n",
    "\n",
    "- Comparatively low predictive accuracy\n",
    "    - Easy to overfit\n",
    "    - Require pruning\n",
    "\n",
    "- High variance\n",
    "    - A small change in the data can cause a large change in the estimated tree.\n",
    "    - Model affected by the rotation of the data\n",
    "\n",
    "---\n",
    "James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An introduction to statistical learning. Vol. 112. New York: springer, 2013.\n",
    "\n",
    "https://github.com/rasbt/stat479-machine-learning-fs19/blob/master/06_trees/06-trees__notes.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Some advantages of decision trees are:\n",
    ">\n",
    "> Simple to understand and to interpret. Trees can be visualised.\n",
    ">\n",
    "> Requires little data preparation. Other techniques often require data normalisation, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values.\n",
    ">\n",
    "> The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\n",
    ">\n",
    "> Able to handle both numerical and categorical data. However scikit-learn implementation does not support categorical variables for now. Other techniques are usually specialised in analysing datasets that have only one type of variable. See algorithms for more information.\n",
    ">\n",
    "> Able to handle multi-output problems.\n",
    ">\n",
    "> Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.\n",
    ">\n",
    "> Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.\n",
    ">\n",
    "> Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\n",
    ">\n",
    "> The disadvantages of decision trees include:\n",
    ">\n",
    "> Decision-tree learners can create over-complex trees that do not generalise the data well. This is called overfitting. Mechanisms such as pruning, setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n",
    ">\n",
    "> Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n",
    ">\n",
    "> Predictions of decision trees are neither smooth nor continuous, but piecewise constant approximations as seen in the above figure. Therefore, they are not good at extrapolation.\n",
    ">\n",
    "> The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.\n",
    ">\n",
    ">There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n",
    ">\n",
    "> Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/tree.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As discussed earlier, the parameters that control model complexity in decision trees are the pre-pruning parameters that stop the building of the tree before it is fully developed. Usually, picking one of the pre-pruning strategies—setting either max_depth, max_leaf_nodes, or min_samples_leaf—is sufficient to prevent overfit‐ ting.\n",
    "> Decision trees have two advantages over many of the algorithms we’ve discussed so far: the resulting model can easily be visualized and understood by nonexperts (at least for smaller trees), and the algorithms are completely invariant to scaling of the data. As each feature is processed separately, and the possible splits of the data don’t depend on scaling, no preprocessing like normalization or standardization of features is needed for decision tree algorithms. In particular, decision trees work well when you have features that are on completely different scales, or a mix of binary and con‐ tinuous features.\n",
    "> The main downside of decision trees is that even with the use of pre-pruning, they tend to overfit and provide poor generalization performance. Therefore, in most applications, the ensemble methods we discuss next are usually used in place of a single decision tree.\n",
    "\n",
    "Müller, A. C., & Guido, S. (2016). Introduction to machine learning with Python: a guide for data scientists. \" O'Reilly Media, Inc.\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTES__\n",
    "\n",
    "- Decision trees potentially more mirror human decision-making than the regression and classification approaches previously discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (+) Easy to explain\n",
    "\n",
    "As well as being able to plot them in a straight forward manner, decision trees allow us assess the _importance_ of each feature for classifying the data.\n",
    "\n",
    "The importance (or Gini importance) of a feature is the normalized total reduction of the criterion (e.g. Gini) brought by that feature.\n",
    "\n",
    "---\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "__Notes__\n",
    "- Lets fit all the penguins on the full data (without categories) and see what features are used to split the data and where they are in the tree. \n",
    "- For bagging regression trees we can use the RSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_g = DecisionTreeClassifier(criterion='gini',\n",
    "                               random_state=42)\n",
    "#DT_g.fit(datasets['bin']['X'], datasets['bin']['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_feat_import(DT, X, y, feat_names, class_names, title):\n",
    "\n",
    "    DT.fit(X, y)\n",
    "    # get the importances for the features\n",
    "    importances = DT.feature_importances_\n",
    "\n",
    "    importances_series = pd.Series(importances,index=feat_names).sort_values(ascending = False)\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows = 2, figsize=(8,8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    with plt.style.context(\"classic\"):\n",
    "        plt.sca(axes[0])\n",
    "        tp = tree.plot_tree(DT,\n",
    "                       feature_names=feat_names, \n",
    "                       class_names=class_names,\n",
    "                       filled = True)\n",
    "    \n",
    "    plt.sca(axes[1])\n",
    "    # plot the important features\n",
    "    importances_series.plot.barh(legend =False, grid=False)\n",
    "    plt.title(title)\n",
    "\n",
    "    #plt.xticks(rotation=45,ha='right')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    #plt.savefig('forest_importances.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # summarize feature importance\n",
    "    for i,v in enumerate(importances):\n",
    "        print(color.BOLD+feat_names[i]+color.END+\": %.3f\" % (v))\n",
    "\n",
    "    print(color.BOLD+\"total: \"+color.END + str(round(sum(importances),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_feat_import(DT_g, datasets['bin']['X'], datasets['bin']['y'], \n",
    "                 datasets['bin']['feats'],datasets['bin']['class'],\n",
    "                 'Feature Importances for Classifying Adelie and Gentoo Penguins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DTREEVIS:\n",
    "    viz = dtreeviz(DT_g, datasets['bin']['X'], datasets['bin']['y'], \n",
    "                   feature_names=datasets['bin']['feats'],\n",
    "                   class_names=datasets['bin']['class'],\n",
    "                   orientation ='LR', \n",
    "                   colors = col_dict, # doesnt seem to do much..\n",
    "                   scale=2.0\n",
    "                  )\n",
    "    display(viz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (+) Inherently multiclass\n",
    "Tree-based classifiers are inherently multiclass...\n",
    "\n",
    "__[Insert about multiclass]__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_g.fit(datasets['multi']['X'], datasets['multi']['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "names = [\"Sex\", \"Length\", \"Diameter\", \"Height\", \"Whole weight\", \n",
    "         \"Shucked weight\", \"Viscera weight\", \"Shell weight\", \"Rings\"]\n",
    "\n",
    "df = pd.read_csv(\"abalone_data.csv\", names=names)\n",
    "y_labels = df[\"Sex\"]\n",
    "X = df.drop(\"Sex\", axis=1)\n",
    "\n",
    "# create a dictionary with the our int labels\n",
    "labels_multi = dict(zip(y_labels.unique(), range(3)))\n",
    "\n",
    "# make a binary version - infants vs. adults\n",
    "labels_bin = labels_multi.copy()\n",
    "labels_bin['F'] = 0; labels_bin['I'] = 1\n",
    "\n",
    "# replace the labels so they are now binary\n",
    "y_bin = y_labels.replace(labels_bin)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y_bin.values, test_size = 0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(10,5))\n",
    "with plt.style.context(\"classic\"):\n",
    "    tree.plot_tree(DT_g,\n",
    "                   feature_names=datasets['multi']['feats'], \n",
    "                   class_names=datasets['multi']['class'], \n",
    "                   filled = True)\n",
    "    plt.show()\n",
    "print(datasets['multi']['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Extra__\n",
    "\n",
    "Unsurprisingly, more features are needed to separate out the multi-class problem than in the binary class as can be seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_feat_import(DT_g, datasets['multi']['X'], datasets['multi']['y'], \n",
    "                 datasets['multi']['feats'], \n",
    "                 'Feature Importances for Classifying Adelie, Gentoo, and Chinstrap Penguins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: Categorical Features and Sklearn\n",
    "\n",
    "You may also be wondering: where are my previous data visualisations of the categorical data before this? Well Sklearn's CART decision trees currently _\"does not support categorical variables\"_. This means:\n",
    "\n",
    "- Do not use `Label Encoding` if your categorical data is __not ordinal__ with `DecisionTreeClassifier()`, you'll end up with splits that do not make sense, as the data will be treat as numeric<sup>Web1</sup>.\n",
    "\n",
    "- Using a `OneHotEncoder` is the only current valid way with sklearn, allowing arbitrary splits not dependent on the label ordering, but is computationally expensive and it can deteriorate the performance of decision trees as it leads to sparse features, which can mess up feature importance<sup>Web1</sup>.\n",
    "\n",
    "__Solution__\n",
    "\n",
    "Currently the best way of handling categorical features is to use `catboost`. `catboost` is a boosting classifier as we discuss later, which can handle categorical inputs. If we want a tree then we can do something like the example below. However, while making these materials I haven't used `catboost` much so I'm still trying to figure out how to understand the categorical splits using this package - so I'll have to leave you to do your own digging to understand the tree below.\n",
    "\n",
    "---\n",
    "Web1. https://stackoverflow.com/questions/38108832/passing-categorical-data-to-sklearn-decision-tree\n",
    "Web2. https://scikit-learn.org/stable/modules/tree.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "from sklearn.metrics import accuracy_score\n",
    "test = penguins_rm\n",
    "X = test.drop('species', axis=1)[[\"sex\", \"island\"]]\n",
    "y = test['species']\n",
    "\n",
    "categorical_features_indices = np.where(X.dtypes != np.float)[0]\n",
    "\n",
    "pool = Pool(X, y, cat_features=categorical_features_indices, feature_names=list(X.columns))\n",
    "\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    custom_loss=['Accuracy'],\n",
    "    random_seed=42,\n",
    "    logging_level='Info',\n",
    "    iterations = 1,\n",
    "    max_depth=2,\n",
    "    max_ctr_complexity = 0,\n",
    ")\n",
    "\n",
    "model.fit(pool,\n",
    ")\n",
    "\n",
    "plot = model.plot_tree(tree_idx=0, pool = pool)\n",
    "\n",
    "display(plot)\n",
    "\n",
    "print(\"Leaf 1 prediction: \" + str(np.argmax([0.830,-0.415, -0.415])))\n",
    "print(\"Leaf 2 prediction: \" + str(np.argmax([0,0,0])))\n",
    "print(\"Leaf 3 prediction: \" + str(np.argmax([0.169, 0.255,-0.424])))\n",
    "print(\"Leaf 4 prediction: \" + str(np.argmax([-0.088, -0.2473,0.561])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (-) High variance\n",
    "\n",
    "__TODO__\n",
    "- more demonstration of this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rotation of the Data\n",
    "As can be seen by the descision boundary, a decision tree is quite boxy. Furthermore, how the model makes a decision boundary is going to be affected by the rotation of the data (as DTs create straight lines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_tree(DT_g1, datasets['blbd']['X'], datasets['blbd']['y'], datasets['blbd']['feats'], \n",
    "             datasets['blbd']['class'], [[37, 22],[51.5, 22]], r_labels, tp_labels)\n",
    "regions_tree(DT_g, datasets['blbd']['X'], datasets['blbd']['y'], datasets['blbd']['feats'], \n",
    "             datasets['blbd']['class'], [[57.5, 19.5],[57.5, 14.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is a highly non-linear and complex relationship between the features and the response then decision trees may outperform classical approaches.\n",
    "\n",
    "However if the relationship between the features and the response is well approximated by a linear model, then an approach such as linear regression will likely work well.\n",
    "\n",
    "---\n",
    "James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An introduction to statistical learning. Vol. 112. New York: springer, 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Notes__\n",
    "\n",
    "- Linear regression assumes a model of the form $f(X)=\\beta 0+\\sum_{j=1}^p X_j\\beta_j$,\n",
    "\n",
    "- Regression trees assume a model of the form $f(X)=\\sum_{m=1}^Mc_m\\cdot1_{(X\\in R_m)}$, where $R_1, …, R_M$ represent a partition of feature space. \n",
    "\n",
    "- The relative performances of tree-based and classical approaches can be assessed by estimating the test error, using either cross-validation or the validation set approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6df1eeb7aba6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"fig_8-7.png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "Image(os.path.join(\"fig_8-7.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forests\n",
    "\n",
    "relatively good performance “out of the box” and ease of use (not much tuning required to get good results) https://github.com/rasbt/stat451-machine-learning-fs20/blob/master/L07/07-ensembles__notes.pdf.\n",
    "\n",
    "Advantage:\n",
    "- _\"Random forests are an effective tool in prediction. Because of the Law of Large Numbers they do not overfit. Injecting the right kind of randomness makes them accurate classifiers and regressors\"_ Breiman L: Random forests. Machine Learning 2001, 45: 5–32.\n",
    "    - although note the above statement has been questioned in Segal MR: Machine Learning Benchmarks and Random Forest Regression. Technical Report, Center for Bioinformatics & Molecular Biostatistics, University of California, San Francisco 2004.\n",
    "    \n",
    "An advantage of ExtraTrees is that it is faster than random forests because its time consuming to to find the best theshold for each feature at each node and it doesnt need to do that (hands on ML).\n",
    "\n",
    "Disadvantage:\n",
    "- Not good for very high-dimensional sparse data<sup>1</sup>.\n",
    "\n",
    "---\n",
    "1. Müller, A. C., & Guido, S. (2016). Introduction to machine learning with Python: a guide for data scientists. \" O'Reilly Media, Inc.\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Random forests for regression and classifica‐ tion are currently among the most widely used machine learning methods. They are very powerful, often work well without heavy tuning of the parameters, and don’t require scaling of the data.\n",
    "Essentially, random forests share all of the benefits of decision trees, while making up for some of their deficiencies. One reason to still use decision trees is if you need a compact representation of the decision-making process. It is basically impossible to interpret tens or hundreds of trees in detail, and trees in random forests tend to be deeper than decision trees (because of the use of feature subsets). Therefore, if you need to summarize the prediction making in a visual way to nonexperts, a single decision tree might be a better choice. While building random forests on large data‐ sets might be somewhat time consuming, it can be parallelized across multiple CPU cores within a computer easily. If you are using a multi-core processor (as nearly all modern computers do), you can use the n_jobs parameter to adjust the number of cores to use. Using more CPU cores will result in linear speed-ups (using two cores, the training of the random forest will be twice as fast), but specifying n_jobs larger than the number of cores will not help. You can set n_jobs=-1 to use all the cores in your computer. You should keep in mind that random forests, by their nature, are random, and set‐ ting different random states (or not setting the random_state at all) can drastically change the model that is built. The more trees there are in the forest, the more robust it will be against the choice of random state. If you want to have reproducible results, it is important to fix the random_state. Random forests don’t tend to perform well on very high dimensional, sparse data, such as text data. For this kind of data, linear models might be more appropriate. Random forests usually work well even on very large datasets, and training can easily be parallelized over many CPU cores within a powerful computer. However, random forests require more memory and are slower to train and to predict than linear mod‐ els. If time and memory are important in an application, it might make sense to use a linear model instead.\n",
    "\n",
    "Müller, A. C., & Guido, S. (2016). Introduction to machine learning with Python: a guide for data scientists. \" O'Reilly Media, Inc.\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at how a descion boundary created by a bagged tree could generalise better than a single tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_search(model, params, X, y, save_path, n_iter=60, random_state=42, overwrite=False):\n",
    "    if os.path.exists(save_path) and overwrite==False:\n",
    "        #load the model\n",
    "        models = joblib.load(save_path)\n",
    "    else:\n",
    "        # check all param inputs are lists\n",
    "        if all(type(x)==list for x in params.values()):\n",
    "            search_type = \"Gridsearch\"\n",
    "            models = GridSearchCV(model, param_grid=params)\n",
    "            n_iter = len(list(itertools.product(*list(iter(params.values())))))\n",
    "        else:\n",
    "            search_type = \"Randomsearch\"\n",
    "            models = RandomizedSearchCV(model, param_distributions=params,\n",
    "                                        n_iter=n_iter, random_state=random_state)\n",
    "        \n",
    "        start = time()\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            models.fit(X, y)\n",
    "        \n",
    "        print(search_type + \" took %.2f seconds for %d candidates\" % ((time() - start), n_iter))\n",
    "        joblib.dump(models, save_path)\n",
    "    \n",
    "    return models\n",
    "\n",
    "cancer_features = ['mean radius','mean smoothness']\n",
    "# specify parameters and distributions to sample from\n",
    "param_grid = {\"min_samples_leaf\":list(range(1,15))}\n",
    "\n",
    "lsamples_gs = hyper_search(DT, param_grid, X[cancer_features].values, y,\n",
    "                           os.path.join(os.getcwd(), \"Models\", \"lsamples_gs_object.pkl\"))\n",
    "\n",
    "pd.DataFrame(lsamples_gs.cv_results_).sort_values(\"rank_test_score\")[[\"param_min_samples_leaf\", \"mean_test_score\", \"std_test_score\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify parameters and distributions to sample from\n",
    "param_grid = {\"n_estimators\":range(2,1000),\n",
    "              \"max_samples\":uniform(0.05, 1.)}\n",
    "\n",
    "rf_gs = hyper_search(RF, param_grid, X[cancer_features].values, y,\n",
    "                     os.path.join(os.getcwd(), \"Models\", \"rf_gs_object.pkl\"), \n",
    "                     n_iter=15, random_state=1, overwrite=False)\n",
    "\n",
    "pd.DataFrame(rf_gs.cv_results_).sort_values(\"rank_test_score\")[[\"param_n_estimators\", \"param_max_samples\", \"mean_test_score\", \"std_test_score\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dict = {'Tree':lsamples_gs.best_estimator_, 'Forest':rf_gs.best_estimator_}\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "for i, classifier_name in enumerate(tree_dict):\n",
    "    plt.sca(axes[i])\n",
    "\n",
    "    plot_decision_regions(X[cancer_features].values, y.values,\n",
    "                          clf = tree_dict[classifier_name])\n",
    "\n",
    "    plt.xlabel(cancer_features[0]) \n",
    "    plt.ylabel(cancer_features[1])\n",
    "\n",
    "    plt.title(classifier_name)\n",
    "    plt.ylim([0.05,0.18])\n",
    "    plt.xlim([10,20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Dimension Reduction\n",
    "#### Model Stacking\n",
    "A method growing in popularity is to use model stacking, where the input to one model is the output of another. This allows for nonlinearities to be captured in the first model, and the potential to use a simple linear model as the last layer. Deep learning is an example of model stacking as, often neural networks are layered on top of one another, to optimize both the features and the classifier simultaneously<sup>1</sup>.\n",
    "\n",
    "---\n",
    "\n",
    "1. Zheng, A., & Casari, A. (2018). Feature Engineering for Machine Learning: Principles and Techniques for Data Scientists. \" O'Reilly Media, Inc.\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Feature Importances\n",
    "\n",
    "An example of model stacking is to use the output of a decision tree–type model as input to a linear classifier. We can gain the importance for each feature by getting the average impurity decrease computed from all decision trees in the forest without regarding the linear separability of the classes. However, if features are highly correlated, one feature may be ranked highly while the information of the others not being fully captured<sup>1</sup>. \n",
    "\n",
    "---\n",
    "\n",
    "1. Raschka, Sebastian, and Vahid Mirjalili. Python Machine Learning, 2nd Ed. Packt Publishing, 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/explaining-feature-importance-by-example-of-a-random-forest-d9166011959e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Rather than manually setting a theshold like we have done (looking at the top 30) we can put it in a pipeline and use the SelectFromModel function from Scikit-learn. Using this we can still provide both a numeric theshold or we could use a heuristic such as the mean and median<sup>1</sup>.\n",
    "\n",
    "---\n",
    "\n",
    "1. http://scikit-learn.org/stable/modules/feature_selection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[4mSVM\u001b[0m\n",
      "CV accuracy: 0.906 +/- 0.039\n",
      "\u001b[1m\u001b[4mForest SVM\u001b[0m\n",
      "CV accuracy: 0.986 +/- 0.012\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "svm = SVC(kernel='rbf', random_state=42, class_weight = 'balanced')\n",
    "rf = RandomForestClassifier(criterion='gini',\n",
    "                            n_estimators=100,\n",
    "                            max_features = 'sqrt',\n",
    "                            random_state=42,\n",
    "                            class_weight = 'balanced',\n",
    "                            n_jobs=-1)\n",
    "\n",
    "rf_svm = Pipeline([\n",
    "  ('feature_selection', SelectFromModel(rf, threshold = 'mean')),\n",
    "  ('classification', svm)\n",
    "])\n",
    "\n",
    "svm_dict = {'SVM':svm, 'Forest SVM':rf_svm}\n",
    "\n",
    "for classifier_name in svm_dict:\n",
    "    scores = cross_val_score(estimator=svm_dict[classifier_name], \n",
    "                             X=X_train, \n",
    "                             y=y_train, \n",
    "                             scoring = 'accuracy',\n",
    "                             cv=StratifiedKFold(),\n",
    "                             n_jobs=-1)\n",
    "\n",
    "    print(color.BOLD+color.UNDERLINE+classifier_name+color.END)\n",
    "    #print('CV accuracy scores: %s' % scores)\n",
    "    print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation Importance\n",
    "\n",
    "_\"impurity-based feature importances can be misleading for high cardinality features (many unique values). See `sklearn.inspection.permutation_importance` as an alternative\"_ https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "- _\"The permutation feature importance is defined to be the decrease in a model score when a single feature value is randomly shuffled<sup>9</sup>. This procedure breaks the relationship between the feature and the target, thus the drop in the model score is indicative of how much the model depends on the feature. This technique benefits from being model agnostic and can be calculated many times with different permutations of the feature.\"_ https://scikit-learn.org/stable/modules/permutation_importance.html\n",
    "- _\"Its validation performance, measured via the score, is significantly larger than the chance level. This makes it possible to use the permutation_importance function to probe which features are most predictive\"_ https://scikit-learn.org/stable/modules/permutation_importance.html\n",
    "-_\"Using a held-out set makes it possible to highlight which features contribute the most to the generalization power of the inspected model. Features that are important on the training set but not on the held-out set might cause the model to overfit.\"_ https://scikit-learn.org/stable/modules/permutation_importance.html\n",
    "---\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTES__\n",
    "- _\"Warning: Features that are deemed of low importance for a bad model (low cross-validation score) could be very important for a good model. Therefore it is always important to evaluate the predictive power of a model using a held-out set (or better with cross-validation) prior to computing importances. Permutation importance does not reflect to the intrinsic predictive value of a feature by itself but how important this feature is for a particular model.\"_ https://scikit-learn.org/stable/modules/permutation_importance.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "_\"As concluding remarks about ensemble techniques, it is worth noting that ensemble learning increases the computational complexity compared to individual classifiers. In practice, we need to think carefully about whether we want to pay the price of increased computational costs for an often relatively modest improvement in predictive performance._\n",
    "\n",
    "_An often-cited example of this tradeoff is the famous \\$1 million Netflix Prize, which was won using ensemble techniques. The details about the algorithm were published in The BigChaos Solution to the Netflix Grand Prize by A. Toescher, M. Jahrer, and R. M. Bell, Netflix Prize documentation, 2009, which is available at http://www.stat.osu.edu/~dmsl/GrandPrize2009_BPC_BigChaos.pdf. The winning team received the $1 million grand prize money; however, Netflix never implemented their model due to its complexity, which made it infeasible for a real-world application:_\n",
    "\n",
    "_\"We evaluated some of the new methods offline but the additional accuracy gains that we measured did not seem to justify the engineering effort needed to bring them into a production environment.\" http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html\"_ Python Machine Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "1. James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An introduction to statistical learning. Vol. 112. New York: springer, 2013.\n",
    "2. Gorman KB, Williams TD, Fraser WR (2014). Ecological sexual dimorphism and environmental variability within a community of Antarctic penguins (genus Pygoscelis). PLoS ONE 9(3):e90081. https://doi.org/10.1371/journal.pone.0090081\n",
    "3. Géron, A. (2017). Hands-on machine learning with Scikit-Learn and TensorFlow: concepts, tools, and techniques to build intelligent systems. \" O'Reilly Media, Inc.\".\n",
    "4. Raschka, Sebastian, and Vahid Mirjalili. Python Machine Learning, 2nd Ed. Packt Publishing, 2017."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "authorship_tag": "ABX9TyMNuZgSJ2BiDt5YMpOB66EK",
   "collapsed_sections": [],
   "name": "Clustering.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
